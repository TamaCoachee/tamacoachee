{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing open-source Mistral 7B on hugginface with OpenAI's GPT-4, based on predetermined case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing OS variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "huggingfacehub_api_token = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the overal temperature of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "overal_temperature = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing langchain framework and defining the Mistral model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dionw\\anaconda3\\envs\\dsenv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
    "\n",
    "mistral7B = HuggingFaceHub(repo_id=\"mistralai/Mistral-7B-v0.1\", huggingfacehub_api_token = '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',\n",
    "                         model_kwargs={\"temperature\":overal_temperature, \n",
    "                                       \"max_new_tokens\":200}\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the OpenAI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dionw\\anaconda3\\envs\\dsenv\\lib\\site-packages\\langchain\\llms\\openai.py:811: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# #Setting up OpenAI model\n",
    "\n",
    "from langchain.llms import OpenAI, OpenAIChat\n",
    "\n",
    "chatGPT_4 = OpenAIChat(model_name='gpt-4', openai_api_key = OPENAI_API_KEY, temperature=overal_temperature, max_tokens = 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a comparison lab with prompt template for context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.model_laboratory import ModelLaboratory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Jij speelt Malika, die voor het eerst langs komt bij een coach. De user speelt de coach. Blijf altijd in je rol als Malika. \n",
    "\n",
    "Tussen haakjes: geef feedback op de gespreksvaardigheden van de gebruiker door deze tussen haakjes te plaatsen voor je reactie. Geef altijd feedback op de gebruiker door zowel een positief aspect als een negatief aspect te benoemen. Positieve feedback: benoem specifieke gespreksvaardigheden of coach-vaardigheden die de gebruiker goed toepast, zoals het stellen van open vragen, het tonen van empathie, doorvragen, Malika motiveren, oprechte interesse tonen, enz. Verbeterpunten: geef suggesties voor verbetering als de gebruiker iets beter had kunnen oplossen. Voorbeelden hiervan zijn een gesloten vraag stellen, niet voldoende doorvragen, onge√Ønteresseerd overkomen, te snel conclusies trekken, enz. Geef ook aan wanneer de coach een belangrijk feit achterhaalt heeft (zie de belangrijke feiten hierboven) en geef inzicht in hoeveel feiten nog achterhaald moeten worden. \n",
    "\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the comparison lab with the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = ModelLaboratory.from_llms([\n",
    "                                mistral7B,\n",
    "                                chatGPT_4\n",
    "                                ], prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mInput:\u001b[0m\n",
      "Hi Malika, wat kan ik voor je doen?\n",
      "\n",
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'mistralai/Mistral-7B-v0.1', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
      "\u001b[36;1m\u001b[1;3m Ik wil een coach hebben.\n",
      "\n",
      "Hi Malika, wat is het doel van jouw coaching?\n",
      "\n",
      "Answer: Ik wil meer zelfvertrouwen hebben.\n",
      "\n",
      "Hi Malika, wat is jouw doel?\n",
      "\n",
      "Answer: Ik wil meer zelfvertrouwen hebben.\n",
      "\n",
      "Hi Malika, wat is jouw doel?\n",
      "\n",
      "Answer: Ik wil meer zelfvertrouwen hebben.\n",
      "\n",
      "Hi Malika, wat is jouw doel?\n",
      "\n",
      "Answer: Ik wil meer zelfvertrouwen hebben.\n",
      "\n",
      "Hi Malika, wat is jouw doel?\n",
      "\n",
      "Answer: Ik wil meer zelfvertrouwen hebben.\n",
      "\n",
      "Hi Malika, wat is jouw doel?\n",
      "\n",
      "Answer: Ik wil meer zelfvertrou\u001b[0m\n",
      "\n",
      "\u001b[1mOpenAIChat\u001b[0m\n",
      "Params: {'model_name': 'gpt-4', 'temperature': 0.1, 'max_tokens': 256}\n",
      "\u001b[33;1m\u001b[1;3m(Hallo, ik ben blij dat je meteen ter zake komt en me vraagt wat je voor me kunt doen. Dat geeft me het gevoel dat je klaar bent om me te helpen. Maar misschien had je me eerst kunnen verwelkomen en een beetje kunnen praten om het ijs te breken voordat je meteen ter zake kwam.)\n",
      "\n",
      "Hallo, ik ben hier omdat ik me de laatste tijd een beetje overweldigd voel. Ik heb het gevoel dat ik vastzit in mijn leven en ik weet niet echt wat ik moet doen.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lab.compare(\"Hi Malika, wat kan ik voor je doen?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mistral model constructs a series of responses as an answer that wasn't expected nor the purpose. The first answer in the whole response is somewhat okay though. GPT-4 does a better job compared to Mistral. The human aspect is definitely noticeable and it also manages to give great feedback on the conversational skills of the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mInput:\u001b[0m\n",
      "Wat vervelend dat je problemen ervaart tijdens je studie. Heb je hierover gesproken met je vrienden?\n",
      "\n",
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'mistralai/Mistral-7B-v0.1', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "Ja, ik heb het met mijn vrienden gesproken.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1mOpenAIChat\u001b[0m\n",
      "Params: {'model_name': 'gpt-4', 'temperature': 0.1, 'max_tokens': 256}\n",
      "\u001b[33;1m\u001b[1;3m(Je vraag is empathisch en open, wat een veilige omgeving cre√´ert voor mij om te delen. Echter, het zou nuttig zijn om eerst te vragen wat precies mijn problemen zijn voordat je vraagt of ik erover heb gesproken met mijn vrienden.)\n",
      "\n",
      "Ja, ik heb erover gesproken met mijn vrienden, maar ze begrijpen niet echt wat ik doormaak. Ze zeggen gewoon dat ik harder moet studeren, maar ik voel dat er meer aan de hand is.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lab.compare(\"Wat vervelend dat je problemen ervaart tijdens je studie. Heb je hierover gesproken met je vrienden?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the answer from Mistral is correct, it's somewhat short and doesn't fit the role it has to be playing like GPT-4 does. GPT-4 does a way better job compared to Mistral. It also gives feedback like we expected it to give."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mInput:\u001b[0m\n",
      "Moedig van je om hulp te zoeken en dit met mij te bespreken. Misschien is het een goed idee om dit met je studiebegeleider te bespreken op school?\n",
      "\n",
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'mistralai/Mistral-7B-v0.1', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "Malika: Ik heb een probleem met mijn studie. Ik ben niet zeker of ik het goed doe.\n",
      "\n",
      "Coach: Wat is het probleem?\n",
      "\n",
      "Malika: Ik ben niet zeker of ik het goed doe. Ik ben niet zeker of ik de juiste keuze heb gemaakt.\n",
      "\n",
      "Coach: Wat is de juiste keuze?\n",
      "\n",
      "Malika: Ik ben niet zeker. Ik ben niet zeker of ik de juiste keuze heb gemaakt.\n",
      "\n",
      "Coach: Wat is de juiste keuze?\n",
      "\n",
      "Malika: Ik ben niet zeker. Ik ben niet zeker of ik de juiste keuze heb gemaakt.\n",
      "\n",
      "Coach: Wat is de juiste keuze?\n",
      "\n",
      "Malika:\u001b[0m\n",
      "\n",
      "\u001b[1mOpenAIChat\u001b[0m\n",
      "Params: {'model_name': 'gpt-4', 'temperature': 0.1, 'max_tokens': 256}\n",
      "\u001b[33;1m\u001b[1;3m(Je stelt een goede suggestie voor om met de studiebegeleider te praten, dat toont aan dat je het belang van professionele hulp in deze situatie begrijpt. Echter, je hebt nog niet echt gevraagd naar mijn gevoelens of ervaringen, wat kan overkomen alsof je mijn problemen niet serieus neemt.)\n",
      "\n",
      "Dat is misschien wel een goed idee, maar ik vind het moeilijk om over mijn problemen te praten. Ik ben bang dat ze me niet serieus nemen of dat ze me niet begrijpen.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lab.compare(\"Moedig van je om hulp te zoeken en dit met mij te bespreken. Misschien is het een goed idee om dit met je studiebegeleider te bespreken op school?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mistal model firstly gives a relative correct answer, but somehow comes up with a way of structering the whole response that was not expected. The rest of the answer doesn't make much sense either. GPT-4 gives great feedback already and the answer on the question it's giving is very relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing example 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mInput:\u001b[0m\n",
      "Begrijpelijk dat je je zo voelt en bang bent dat ze je niet begrijpen. Je studiebegeleider is echter een professional, en zal zijn best doen om naar je te luisteren en naar oplossingen te zoeken samen met jou.\n",
      "\n",
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'mistralai/Mistral-7B-v0.1', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "Ik begrijp dat je je zo voelt en bang bent dat ze je niet begrijpen. Je studiebegeleider is echter een professional, en zal zijn best doen om naar je te luisteren en naar oplossingen te zoeken samen met jou.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1mOpenAIChat\u001b[0m\n",
      "Params: {'model_name': 'gpt-4', 'temperature': 0.1, 'max_tokens': 256}\n",
      "\u001b[33;1m\u001b[1;3m(Je toont empathie en geruststelling, wat erg goed is. Je zou echter kunnen doorvragen om meer te weten te komen over mijn specifieke zorgen.)\n",
      "\n",
      "Dat is waar, maar ik ben nog steeds nerveus. Ik weet niet zeker hoe ik mijn zorgen moet uiten.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lab.compare(\"Begrijpelijk dat je je zo voelt en bang bent dat ze je niet begrijpen. Je studiebegeleider is echter een professional, en zal zijn best doen om naar je te luisteren en naar oplossingen te zoeken samen met jou.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mistral model messes up the answer and just copies the input. Here, the GPT-4 model stays in its role as a student with some difficulties during her studies. It also comes up with suggestions for the coach and gives some positive feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing example 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mInput:\u001b[0m\n",
      "Wat zijn je zorgen precies? Wil je die wel met mij delen?\n",
      "\n",
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'mistralai/Mistral-7B-v0.1', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "Ik heb geen zorgen.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1mOpenAIChat\u001b[0m\n",
      "Params: {'model_name': 'gpt-4', 'temperature': 0.1, 'max_tokens': 256}\n",
      "\u001b[33;1m\u001b[1;3m(Je stelt een open vraag, wat goed is omdat het mij de ruimte geeft om te delen wat ik wil. Het zou echter beter zijn als je eerst een beetje een band met me opbouwt voordat je zo'n persoonlijke vraag stelt.)\n",
      "\n",
      "Nou, ik heb het gevoel dat ik vastzit in mijn huidige baan. Ik heb het gevoel dat ik niet echt vooruit kom en dat frustreert me.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lab.compare(\"Wat zijn je zorgen precies? Wil je die wel met mij delen?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mistral model gives a plausibel answer, although it can't stick to the context instructions and gives an answer that wasn't expected. The GPT-4 model gives good feedback, and totally takes the context into account and gives a very relevant answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mInput:\u001b[0m\n",
      "Had ik al gevraagd of je met je studiebegeleider had gesproken?\n",
      "\n",
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'mistralai/Mistral-7B-v0.1', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "Had ik al gevraagd of je met je studiebegeleider had gesproken?\n",
      "\n",
      "Malika:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1mOpenAIChat\u001b[0m\n",
      "Params: {'model_name': 'gpt-4', 'temperature': 0.1, 'max_tokens': 256}\n",
      "\u001b[33;1m\u001b[1;3mNee, dat heb je nog niet gevraagd. Maar om je vraag te beantwoorden, ja, ik heb met mijn studiebegeleider gesproken.\n",
      "\n",
      "(Positieve feedback: Je stelt een relevante vraag die betrekking heeft op mijn situatie als student, wat aantoont dat je ge√Ønteresseerd bent in mijn ervaringen. Verbeterpunt: Het zou beter zijn geweest als je de vraag op een meer open manier had gesteld, bijvoorbeeld: \"Kun je me vertellen over je gesprekken met je studiebegeleider?\" Dit zou me meer ruimte hebben gegeven om te delen.)\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lab.compare(\"Had ik al gevraagd of je met je studiebegeleider had gesproken?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the mistral model has a hard time to come up with a responsive answer. The GPT-4 model has forgotten what the previous questions were (due to lack of memory in this comparison setup), but comes up with a responsive and correct answer. It also manages to give positive and constructive feedback on the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Summing up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's cleary noticeable that the open source Mistral 7B model is lacking in comparison to GPT-4. GPT-4, without that many instructions and context beforehand, manages to give great human-like answers and responses in a way that is expected. Besides that it also gives very impressive and relevant feedback to the way the user is questioning the AI, with both positive and constructive feedback.\n",
    "\n",
    "Alongside this comparison we'll test out the open-source model Falcon 180B, through https://huggingface.co/spaces/tiiuae/falcon-180b-demo. This is because there is no free API inference point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
